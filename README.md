# Table of Contents
1. [Introduction](#introduction)

2. [Dataset](#dataset)
    - [Raw Data](#raw-data)
    - [Data Format](#data-format)
    - [Data Curation Methodology](#data-curation-methodology)

3. [Baseline Methods](#baseline-methods)

4. [Evaluation Metrics](#evaluation-metrics)

# Introduction

This repository is for the LGE FoodKG dataset and tools. It contains a custom recipe query dataset curated from [FoodKG](https://foodkg.github.io/), a dataset search tool, baseline methods, and evaluation tools.

# Dataset

## Raw Data

All of the raw recipe data used by curators to create this dataset was accessed through a python-based search tool that was developed to easily search through recipe options. The search tool uses 3 publicly available JSON files: *layer1.json, det_ingrs.json*, and *recipes_with_nutritional_info.json*, originating from the [Recipe1M](http://im2recipe.csail.mit.edu/dataset/login/) dataset. These files are also what the FoodKG dataset is based off of as specified by the [FoodKG dataset construction](https://foodkg.github.io/foodkg.html#:~:text=preparation%20of%20foods.-,Construction,-Prerequisites). Details on how to download the data and setup the search tool can be found in the README under tools/search-tool/.

## Data Format

The format of the dataset is in JSON, where each JSON object consists of the query, query type labels, five options (recipe IDs and text descriptions), the recipe ID of the intended correct answer, and explanations of incorrectness for the other four options. In this way, each group of query and five recipe options form a "multiple-choice" problem. The JSON schema used can be found under data/.

One example of the data:

```json
{
  "query": "I would like meat lasagna but I'm watching my weight",
  "query_type": {
    "Specific": 1,
    "Subjective": 0,
    "Commonsense": 1,
    "Compound": 0,
    "Negated": 0,
    "Analogical": 0,
    "Temporal": 0
  },
  "options": { 
      "34572cc1ee": "Vegetarian lasagna with mushrooms, mixed vegetables, textured vegetable protein, and meat replacement",
      "7042fffd85": "Forgot the Meat Lasagna with onions, mushroom and spinach",
      "0b82f37488": "Beef lasagna with whole wheat noodles, low-fat cottage cheese, and part-skim mozzarella cheese",
      "047ea4e60b": "Cheesy lasagna with Italian sausage, mushrooms, and 8 types of cheese",
      "57139f1a42": "Meat loaf containing vegetables such as potatoes, onions, corn, carrots, and cabbage"
  },
  "answer": "0b82f37488",
  "incorrectness_explanations": {
    "34572cc1ee": "Contains a meat replacement so it's not meat lasagna",
    "7042fffd85": "Doesn't contain any meat so it's not meat lasagna",
    "047ea4e60b": "Not good for watching weight because it has a lot of cheese",
    "57139f1a42": "Is healthy but isn't lasagna"
  },
  "correctness_explanation": {
    "meat lasagna": "Beef lasagna",
    "watching my weight": ["whole-wheat","low-fat","part-skim"]
  }
}
```

## Data Curation Methodology

### Query Generation

A total of 500 queries were generated by five different individuals (100 each). In order to cover a wide range of natural query types, general categories were first brainstormed to guide the query generation. These categories were only used to ensure wide coverage of different query types, maintain natural queries, and avoid overlap of similar queries. They are not used as labels or in any of the evaluation. The categories and the number of queries in each are below:

| Category | Example | Query Count |
| -------- | ------- | ------- |
| Main ingredients (eg. seafood, fruit ) | What are some spinach or kale-based dishes I can make in the oven? | 92 |
| Flavours (eg. spicy, sweet) | I'm craving something sweet and sour, what can I make? | 12 |
| Budget | I need an easy meal under 10 dollars | 17 |
| Specific weather/season/holiday | I'm looking for decorative treats for my Halloween party | 32 |
| Situational (eg. school lunch, party, outdoors) | What's a good lunch I can warm up in the microwave at my office? | 50 |
| Age range (eg. toddlers, adults, seniors) | Could I have a potato recipe for my baby who is teething? | 25 |
| Religion-specific | I want a meal to eat before dawn in Ramadan | 5 |
| Diets and restrictions | I'm allergic to eggs but want to bake a cake | 38 |
| Cooking method | What are ways to cook asparagus if I don't have an oven at home? | 20 |
| Course (eg. breakfast, appetizer, dessert) | I'm focusing on eating healthy, can I get a low calorie hash brown recipe for brunch? | 87 |
| Cuisine type | Can I have an Egyptian dessert? | 100 |

Requirements for queries:

1. Queries should aim to be natural (simulating conversation)
   - To help with this, query generation was guided by the above categories instead of trying to balance the query labels below

2. Every query should have at least two attributes/components
   - eg. "I want a healthy cake" has two parts: "healthy" and "cake", whereas "I want a cake" would not be valid

3. Queries should be in the form of a complete question or statement
   - eg. "Can I have a recipe for lasagna?" as opposed to "lasagna"

4. Queries should incorporate indirect language/require common sense reasoning where possible
   - eg. "I want meat lasagna but I'm watching my weight" instead of "I want healthy meat lasagna"

### Query Type Annotation

After all queries have been generated, each query was labeled according to the query types below, for a total of 11 binary labels where 1/0 indicates true/false. For the first 4 pairs of query types, each query is guaranteed to have one and only one of each pair be labeled "1" or true.

- Specific
  - Specific queries mention a certain dish or recipe name; by default, anything else is considered General

<!--
- Subjective
  - Subjective queries contain something that can be open to more than one interpretation, eg. "healthy"; by default, anything else is considered Objective
-->

- Commonsense
  - Commonsense queries contain terms requiring some level of common sense reasoning, eg. "not greasy"; by default, anything else is considered Direct

<!--
- Compound
  - Compound queries contain compound logic operators such as AND/OR; by default, anything else is considered Simple
-->

- Negated
  - Negated queries contain contradiction or denial of something, eg. using terms like "but not", "without", "doesn't", etc.

- Analogical
  - Analogical queries contain a comparison of the desired recipe to something else

- Temporal
  - Temporal queries contain explicit references to time, eg. time of day, day of week, or using terms like "slow", "fast", "lasting", etc.

| Label | Query Count |
| -------- |-------- |
| Specific | 151 |
| Commonsense | 268 |
| Negated | 109 |
| Analogical | 30 |
| Temporal | 32 |

<!--| Subjective | 186 |-->
<!--| Compound | 73 |-->

Examples of queries and their corresponding labels:

| Query | Specific | Commonsense | Negated | Analogical | Temporal |
| -------- | -------- | -------- | ------- | ------- | -------- |
| I would like a beef recipe but not stew | 0 | 0 | 1 | 0 | 0 |
| I would like a comforting recipe with mushrooms for a cold day | 0 | 1 | 1 | 0 | 0 |
| Can I have a breakfast burrito that will last me until noon? | 1 | 1 | 0 | 0 | 1 |

### Option Generation

After the queries were generated, five recipe options were generated for each query, with one being the correct answer. These options were found with the FoodKG Search Tool. For each of the five options, a text description was generated describing the recipe if the recipe name wasn't descriptive enough (otherwise the recipe name is used as the description). Information for generating these text descriptions comes mainly from the recipe name, ingredients and nutritional information. Sometimes additional recipe details are included such as cooking methodology or estimated time. These text descriptions are designed so they can function as standalone information apart from the information in the full FoodKG knowledge graph.

Requirements for options and descriptions:

1. Each of the four "incorrect" options should be hard negatives or near misses
   - Near misses are defined as options close to the correct answer, differing by as few attributes as possible

2. The text descriptions do not have to contain full recipe details, but need to include enough information to discern the correct answer from the wrong ones

3. The text descriptions should remain factual and avoid any additional inference

4. The text description for the correct answer should avoid direct word-matching with the query as much as possible

### Explanation Generation

In addition to the queries and accompanying options, the data was also augmented with two types of explanations.

1. Explanations of incorrectness (for each of four incorrect options)

    - An short explanation of why each incorrect options is incorrect relative to the given query

2. Explanation of correctness (for the correct option)

    - A mapping of query terms to the corresponding terms in the correct option description that makes the option correct

    - For terms in the query that don't match to an explicit term in the description, the "\<INFERRED\>" tag is used

An example of a query, an incorrect and a correct option, and corresponding explanations is below:

| Query | Option Type | Description | Explanation |
| -------- | ------- | -------- | -------- |
| I want to make a paella but I'm short on time | Incorrect | Paella made with Spanish rice mix, vegetables, and chicken sausage in a slow-cooker | It's made in a slow-cooker so it won't be quick |
| | Correct | Brown rice paella containing mussels and Spanish chorizo that cooks fast | "paella": "paella", "short on time" : "cooks fast" |

### Data Validation

Two full rounds of data validation among curators were conducted. In each round of validation:

- Each subset of 100 queries and the associated labels, options, and descriptions were validated by someone other than the original curator (Validator 1)

- Data was validated to ensure:
  - Queries, options, and text descriptions followed the guidelines established above
  - Correctness and consistency of query type labels
  - A single correct answer can be identified without ambiguity and without looking at the correct answer

- During this validation process, conflicts, issues, and ways to improve were recorded by Validator 1

- A second, different individual (Validator 2) then validated the same subset of data and helped resolve conflicts

- Corrections to the data were then made by the individual originally responsible for that subset of data

# Baseline Methods

A number of different evaluation methods were chosen and run on the data to serve as baseline results. These results also helped standardize the difficulty of queries across different individuals. For each baseline, the accuracy was computed for each query + set of options in aggregate across the whole dataset and individually across different data curators. The code for these baselines can be found under baselines/.

## 1) Overlapping Word Count

The simplest baseline chooses the option description containing the highest number of overlapping words with the query. Prior to this, the option descriptions and query are preprocessed using the [nltk platform](https://www.nltk.org/) for stopword removal and stemming using the [PorterStemmer](https://www.nltk.org/howto/stem.html).

The code can be found in baselines/WordOverlap.ipynb

<!--Results:

| Data Curator | H@1 (accuracy) |
| -------- |-------- |
| K | 17/100 |
| Y | 18/100 |
| H | 17/100 |
| Z | 15/100|
| N | 18/100 |
| **Total** | **85/500 (17%)** |-->

## 2) TF-IDF Ranking

Using the recipe text descriptions as a corpus, each of the five options were ranked according to TF-IDF scoring against the given query by summing up the TF-IDF scores of each query term. This scoring was done on the data after stopword removal and lemmatization were applied using nltk.

- The term frequency used was the logarithmically scaled frequency: $tf(t,d) = log(1+f_{t,d})$ where $f_{t,d}$ is the frequency of term $t$ in document $d$ (the text description of the recipe).

- The inverse document frequency used was adjusted to avoid division-by-zero: $idf(t,D) = log(N/(1 + df))$ where $N$ is the total number of recipe descriptions and $df$ is the document frequency of the term $t$ (number of descriptions containing the term).

<!--The code can be found in baselines/TFIDF.ipynb

Results:

| Data Curator | H@1 (accuracy) |
| -------- |-------- |
| K | 21/100 |
| Y | 14/100 |
| H | 23/100 |
| Z | 25/100|
| N | 20/100 |
| **Total** | **103/500 (20.6%)** |-->

## 4) Neural IR (BERT, TAS-B)

This baseline uses the [TASB Neural IR model](https://huggingface.co/sebastian-hofstaetter/distilbert-dot-tas_b-b256-msmarco) trained on the MSMARCO-Passage dataset to score and rank options. Some background and walkthrough of the methods used can be found in [this blog post](https://python.plainenglish.io/how-to-build-your-own-neural-ir-system-from-scratch-in-python-4a1f30576582).

The code can be found in baselines/TASB.ipynb

<!--Results:

| Data Curator | H@1 (accuracy) |
| -------- |-------- |
| K | 21/100 |
| Y | 20/100 |
| H | 19/100 |
| Z | 8/100|
| N | 16/100 |
| **Total** | **84/500 (16.8%)** |-->

## 4) Zero-Shot QA with 

This baseline applies a pretrained OPT-1.3b language model on the dataset for Question Answering (QA) using the [HuggingFace Transformers](https://arxiv.org/abs/1910.03771) library in Python.

<!--Results:

| Data Curator | H@1 (accuracy) |
| -------- |-------- |
| K | 26/100 |
| Y | 31/100 |
| H | 32/100 |
| Z | 43/100|
| N | 21/100 |
| **Total** | **153/500 (30.6%)** |-->

# Evaluation Metrics

- ## Accuracy (hit@k k=1)