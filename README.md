# Table of Contents
1. [Introduction](#introduction)

2. [Dataset](#dataset)
    - [Raw Data](#raw-data)
    - [Data Format](#data-format)
    - [Data Curation Methodology](#data-curation-methodology)

3. [Baseline Methods](#baseline-methods)


# Introduction

This repository is for the Recipe-MPR dataset and tools. It contains a custom recipe query dataset curated from [FoodKG](https://foodkg.github.io/), a dataset search tool, code for baseline methods, and evaluation tools.

# Recipe-MPR Dataset

The Recipe-MPR data can be found in [data/500QA.json](data/500QA.json).

## Raw Data

All of the raw recipe data used by curators to create this dataset was accessed through a python-based search tool that was developed to easily search through recipe options. The search tool uses 3 publicly available JSON files: *layer1.json, det_ingrs.json*, and *recipes_with_nutritional_info.json*, originating from the [Recipe1M](http://im2recipe.csail.mit.edu/dataset/login/) dataset. These files are also what the FoodKG dataset is based off of as specified by the [FoodKG dataset construction](https://foodkg.github.io/foodkg.html#:~:text=preparation%20of%20foods.-,Construction,-Prerequisites). Details on how to download the data and setup the search tool can be found in [tools/search-tool/README.md](tools/search-tool/README.md).

## Data Format

The format of the dataset is in JSON, where each JSON object consists of the query, five options (recipe IDs and text descriptions), the recipe ID of the intended correct answer, labels of query reasoning strategies, and a mapping from preference aspects in the query to aspects in the correct option text. In this way, each group of query and five recipe options form a "multiple-choice" problem. The JSON schema used can be found under [data/foodkg.schema.json](data/foodkg.schema.json).

One example of the data:

```json
{
  "query": "I would like meat lasagna but I'm watching my weight",
  "query_type": {
    "Specific": 1,
    "Commonsense": 1,
    "Negated": 0,
    "Analogical": 0,
    "Temporal": 0
  },
  "options": { 
      "34572cc1ee": "Vegetarian lasagna with mushrooms, mixed vegetables, textured vegetable protein, and meat replacement",
      "7042fffd85": "Forgot the Meat Lasagna with onions, mushroom and spinach",
      "0b82f37488": "Beef lasagna with whole wheat noodles, low-fat cottage cheese, and part-skim mozzarella cheese",
      "047ea4e60b": "Cheesy lasagna with Italian sausage, mushrooms, and 8 types of cheese",
      "57139f1a42": "Meat loaf containing vegetables such as potatoes, onions, corn, carrots, and cabbage"
  },
  "answer": "0b82f37488",
  "correctness_explanation": {
    "meat lasagna": "Beef lasagna",
    "watching my weight": ["whole-wheat","low-fat","part-skim"]
  }
}
```

## Data Curation Methodology

### Query Generation

A total of 500 queries were generated by five different individuals (100 each). In order to cover a wide range of natural query types, general categories were first brainstormed to guide the query generation. These categories were only used to ensure wide coverage of different query types, maintain natural queries, and avoid overlap of similar queries. They are not used as labels or in any of the evaluation. The categories and the number of queries in each are below:

<!--| Category | Example | Query Count |
| -------- | ------- | ------- |
| Main ingredients (eg. seafood, fruit ) | What are some spinach or kale-based dishes I can make in the oven? | 92 |
| Flavours (eg. spicy, sweet) | I'm craving something sweet and sour, what can I make? | 12 |
| Budget | I need an easy meal under 10 dollars | 17 |
| Specific weather/season/holiday | I'm looking for decorative treats for my Halloween party | 32 |
| Situational (eg. school lunch, party, outdoors) | What's a good lunch I can warm up in the microwave at my office? | 50 |
| Age range (eg. toddlers, adults, seniors) | Could I have a potato recipe for my baby who is teething? | 25 |
| Religion-specific | I want a meal to eat before dawn in Ramadan | 5 |
| Diets and restrictions | I'm allergic to eggs but want to bake a cake | 38 |
| Cooking method | What are ways to cook asparagus if I don't have an oven at home? | 20 |
| Course (eg. breakfast, appetizer, dessert) | I'm focusing on eating healthy, can I get a low calorie hash brown recipe for brunch? | 87 |
| Cuisine type | Can I have an Egyptian dessert? | 100 |-->

Requirements for queries:

1. Queries should aim to be natural (simulating conversation)
   - To help with this, query generation was guided by the above categories instead of trying to balance the query labels below

2. Every query should have at least two attributes/components
   - eg. "I want a healthy cake" has two parts: "healthy" and "cake", whereas "I want a cake" would not be valid

3. Queries should be in the form of a complete question or statement
   - eg. "Can I have a recipe for lasagna?" as opposed to "lasagna"

4. Queries should incorporate indirect language/require common sense reasoning where possible
   - eg. "I want meat lasagna but I'm watching my weight" instead of "I want healthy meat lasagna"

### Query Type Annotation

After all queries have been generated, each query was labeled according to the query types below, for a total of 11 binary labels where 1/0 indicates true/false. For the first 4 pairs of query types, each query is guaranteed to have one and only one of each pair be labeled "1" or true.

- Specific
  - Specific queries mention a certain dish or recipe name; by default, anything else is considered General

<!--
- Subjective
  - Subjective queries contain something that can be open to more than one interpretation, eg. "healthy"; by default, anything else is considered Objective
-->

- Commonsense
  - Commonsense queries contain terms requiring some level of common sense reasoning, eg. "not greasy"; by default, anything else is considered Direct

<!--
- Compound
  - Compound queries contain compound logic operators such as AND/OR; by default, anything else is considered Simple
-->

- Negated
  - Negated queries contain contradiction or denial of something, eg. using terms like "but not", "without", "doesn't", etc.

- Analogical
  - Analogical queries contain a comparison of the desired recipe to something else

- Temporal
  - Temporal queries contain explicit references to time, eg. time of day, day of week, or using terms like "slow", "fast", "lasting", etc.

| Label | Query Count |
| -------- |-------- |
| Specific | 151 |
| Commonsense | 268 |
| Negated | 109 |
| Analogical | 30 |
| Temporal | 32 |

<!--| Subjective | 186 |-->
<!--| Compound | 73 |-->

Examples of queries and their corresponding labels:

| Query | Specific | Commonsense | Negated | Analogical | Temporal |
| -------- | -------- | -------- | ------- | ------- | -------- |
| I would like a beef recipe but not stew | 0 | 0 | 1 | 0 | 0 |
| I would like a comforting recipe with mushrooms for a cold day | 0 | 1 | 1 | 0 | 0 |
| Can I have a breakfast burrito that will last me until noon? | 1 | 1 | 0 | 0 | 1 |

### Option Generation

After the queries were generated, five recipe options were generated for each query, with one being the correct answer. These options were found with the FoodKG Search Tool. For each of the five options, a text description was generated describing the recipe if the recipe name wasn't descriptive enough (otherwise the recipe name is used as the description). Information for generating these text descriptions comes mainly from the recipe name, ingredients and nutritional information. Sometimes additional recipe details are included such as cooking methodology or estimated time. These text descriptions are designed so they can function as standalone information apart from the information in the full FoodKG knowledge graph.

Requirements for options and descriptions:

1. Each of the four "incorrect" options should be hard negatives or near misses
   - Near misses are defined as options close to the correct answer, differing by as few attributes as possible

2. The text descriptions do not have to contain full recipe details, but need to include enough information to discern the correct answer from the wrong ones

3. The text descriptions should remain factual and avoid any additional inference

4. The text description for the correct answer should avoid direct word-matching with the query as much as possible

### Explanation Generation

In addition to the queries and accompanying options, the data was also augmented with two types of explanations.

1. Explanations of incorrectness (for each of four incorrect options)

    - An short explanation of why each incorrect options is incorrect relative to the given query

2. Explanation of correctness (for the correct option)

    - A mapping of query terms to the corresponding terms in the correct option description that makes the option correct

    - For terms in the query that don't match to an explicit term in the description, the "\<INFERRED\>" tag is used

An example of a query, an incorrect and a correct option, and corresponding explanations is below:

| Query | Option Type | Description | Explanation |
| -------- | ------- | -------- | -------- |
| I want to make a paella but I'm short on time | Incorrect | Paella made with Spanish rice mix, vegetables, and chicken sausage in a slow-cooker | It's made in a slow-cooker so it won't be quick |
| | Correct | Brown rice paella containing mussels and Spanish chorizo that cooks fast | "paella": "paella", "short on time" : "cooks fast" |

### Data Validation

Two full rounds of data validation among curators were conducted. In each round of validation:

- Each subset of 100 queries and the associated labels, options, and descriptions were validated by someone other than the original curator (Validator 1)

- Data was validated to ensure:
  - Queries, options, and text descriptions followed the guidelines established above
  - Correctness and consistency of query type labels
  - A single correct answer can be identified without ambiguity and without looking at the correct answer

- During this validation process, conflicts, issues, and ways to improve were recorded by Validator 1

- A second, different individual (Validator 2) then validated the same subset of data and helped resolve conflicts

- Corrections to the data were then made by the individual originally responsible for that subset of data

# Baseline Methods

A number of different off-the-shelf baseline evaluation methods ranging from Sparse (OWC, TF-IDF, BM25), Dense ([BERT](https://huggingface.co/bert-base-uncased), [TAS-B](https://huggingface.co/sebastian-hofstaetter/distilbert-dot-tas_b-b256-msmarco), [GPT-3 embeddings](https://platform.openai.com/docs/api-reference/embeddings)), Zero-Shot ([GPT-2](https://huggingface.co/gpt2), [OPT](https://huggingface.co/docs/transformers/model_doc/opt)), and Few-Shot (GPT-2, OPT) are provided. All baseline methods are provided for two evaluation settings: *Monolithic* and *Aspect-based*. Results from the baselines are automatically saved to .csv files with accuracy (hit@1) as the metric.

- Monolithic setting: see [baselines/monolithic/](baselines/monolithic/) for evaluation code

- Aspect-based setting: see [baselines/aspects/](baselines/aspects/) for evaluation code

To selectively run evaluation experiments, the [baselines/config.json](baselines/config.json) file can be modified and the shell scripts found under [baselines/scripts/](baselines/scripts/) can be run directly. For example,

```
./single_runs.sh
```

To create random splits of the data into k-folds, the [utils/make_fold_inds.py](utils/make_fold_inds.py) script can be run to output the fold indices into a JSON file. The fold indices used in our experiments are found under [baselines/folds/](baselines/folds).

```
python3 make_fold_inds.py -K [# of folds]
```

The [utils/generate_results.py](utils/generate_results.py) script is also provided, which contains functions to parse through the .csv results files from the evaluation code and generates a dataframe of all results for easier analysis and comparison of results.
