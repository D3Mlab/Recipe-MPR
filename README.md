# Table of Contents
1. [Introduction](#introduction)

2. [Dataset](#dataset)
    - [Data Format](#data-format)
    - [Data Curation Methodology](#data-curation-methodology)

3. [Baseline Methods](#baseline-methods)

4. [Evaluation Metrics](#evaluation-metrics)

# Introduction

This repository is for the LGE FoodKG dataset and tools. It contains a custom recipe query dataset curated from [FoodKG](https://foodkg.github.io/), a dataset search tool, baseline methods, and evaluation tools.

# Dataset

## Data Format

The format of the dataset is in JSON, where each JSON object consists of the query, query type labels, five options (recipe IDs and text descriptions), the recipe ID of the intended correct answer, and explanations of incorrectness for the other four options. In this way, each group of query and five recipe options form a "multiple-choice" problem. The JSON schema used can be found under data/.

One example of the data:

```json
{
  "query": "I would like meat lasagna but I'm watching my weight",
  "query_type:": {
    "General": 0,
    "Specific": 1,
    "Objective": 1,
    "Subjective": 0,
    "Indirect": 1,
    "Direct": 0,
    "Simple": 1,
    "Compound": 0,
    "Negated": 0,
    "Analogical": 0,
    "Temporal": 0
  },
  "options": { 
      "34572cc1ee": "Vegetarian lasagna with mushrooms, mixed vegetables, textured vegetable protein, and meat replacement",
      "7042fffd85": "Forgot the Meat Lasagna with onions, mushroom and spinach",
      "0b82f37488": "Beef lasagna with whole wheat noodles, low-fat cottage cheese, and part-skim mozzarella cheese",
      "047ea4e60b": "Cheesy lasagna with Italian sausage, mushrooms, and 8 types of cheese",
      "57139f1a42": "Meat loaf containing vegetables such as potatoes, onions, corn, carrots, and cabbage"
  },
  "answer": "0b82f37488",
  "explanations": {
    "34572cc1ee": "Contains a meat replacement so it's not meat lasagna",
    "7042fffd85": "Doesn't contain any meat so it's not meat lasagna",
    "047ea4e60b": "Not good for watching weight because it has a lot of cheese",
    "57139f1a42": "Is healthy but isn't lasagna"
  }
}
```

## Data Curation Methodology

### FoodKG Search Tool

A python-based search tool was developed to easily search through recipes in the FoodKG dataset when generating options for each query. It searched through the dataset using text matching to the recipe names and had the option to include/exclude certain ingredients. It returns a display of the recipe ID, recipe name, ingredients, and nutritional information for up to 50 recipes. See README under tools/foodkg-search-tool for more information on setup and usage.

### Query Generation

A total of 500 queries were generated by five different individuals (100 each). In order to cover a wide range of natural query types, general categories were first brainstormed to guide the query generation. These categories were only used to ensure wide coverage of different query types, maintain natural queries, and avoid overlap of similar queries. They are not used as labels or in any of the evaluation. The categories and the number of queries in each are below:

| Category | Count   |
| -------- | ------- |
| Varying main ingredients (eg. seafood, fruit ) | 92 |
| Varying flavours (eg. spicy, sweet) | 12 |
| Varying budget | 17 |
| Specific weather/season/holiday | 32 |
| Situational (eg. school lunch, party, outdoors) | 50 |
| Varying age range (eg. toddlers, adults, seniors) | 25 |
| Religion-specific | 5 |
| Diets and restrictions | 38 |
| Varying cooking method | 20 |
| Varying course (eg. breakfast, appetizer, dessert) | 87 |
| Varying cuisine type | 100 |

Requirements for queries:

1. Queries should aim to be natural (simulating conversation)
   - To help with this, query generation was guided by the above categories instead of trying to balance the query labels below

2. Every query should have at least two attributes/components
   - eg. "I want a healthy cake" has two parts: "healthy" and "cake", whereas "I want a cake" would not be valid

3. Queries should be in the form of a complete question or statement
   - eg. "Can I have a recipe for lasagna?" as opposed to "lasagna"

4. Queries should incorporate indirect language/require common sense reasoning where possible
   - eg. "I want meat lasagna but I'm watching my weight" instead of "I want healthy meat lasagna"


### Query Type Annotation

After all queries have been generated, each query was labeled according to the query types below, for a total of 11 binary labels where 1/0 indicates true/false. For the first 4 pairs of query types, each query is guaranteed to have one and only one of each pair be labeled "1" or true.

- General vs. Specific
  - Specific queries mention a certain dish or recipe name; anything else is considered General

- Objective vs. Subjective
  - Subjective queries contain something that can be open to more than one interpretation, eg. "healthy"; anything else is considered Objective

- Indirect vs. Direct (common sense involved)
  - Indirect queries contain terms requiring some level of common sense reasoning, eg. "not greasy"; anything else is considered Direct

- Simple vs. Compound Logic
  - Compound queries contain compound logic operators such as AND/OR; anything else is considered Simple

- Negated
  - Queries are negated if there exists contradiction or denial of something, eg. using terms like "but not", "without", "doesn't", etc.

- Analogical
  - Analogical queries contain a comparison of the desired recipe to something else

- Temporal
  - Temporal queries contain explicit references to time, eg. time of day, or using terms like "slow", "fast", "lasting", etc.

[add count of each type?]

Examples of queries and their corresponding labels:

| Query | General | Specific | Objective | Subjective | Indirect | Direct | Simple | Compound | Negated | Analogical | Temporal |
| -------- | ------- | -------- | ------- | -------- | ------- | -------- | ------- | -------- | ------- | -------- | ------- |
| I would like a beef recipe but not stew | 1 | 0 | 1 | 0 | 0 | 1 | 1 | 0 | 1 | 0 | 0 |
| I want to make fries like the ones from McDonalds | 0 | 1 | 0 | 1 | 0 | 1 | 1 | 0 | 0 | 1 | 0 |
| Can I have a breakfast burrito that will last me until noon? | 0 | 1 | 0 | 1 | 1 | 0 | 1 | 0 | 0 | 0 | 1 |

### Option Generation

After the queries were generated, five recipe options were generated for each query, with one being the correct answer. These options were found with the FoodKG Search Tool. For each of the five options, a text description was generated describing the recipe if the recipe name wasn't descriptive enough (otherwise the recipe name is used as the description). Information for generating these text descriptions comes mainly from the recipe name, ingredients and nutritional information. Sometimes additional recipe details are included such as cooking methodology or estimated time. These text descriptions are designed so they can function as standalone information apart from the information in the full FoodKG knowledge graph.

Requirements for options and descriptions:

1. Each of the four "incorrect" options should be hard negatives or near misses
   - Near misses are defined as options close to the correct answer, differing by as few attributes as possible

2. The text descriptions do not have to contain full recipe details, but need to include enough information to discern the correct answer from the wrong ones

3. The text descriptions should remain factual and avoid any additional inference

4. The text description for the correct answer should avoid direct word-matching with the query as much as possible

Each incorrect option was augmented with an additional brief explanation for why the option is incorrect. An example of a query, incorrect option, and explanation is below:

| Query | Option Description | Explanation |
| -------- | ------- | -------- |
| I want to make a paella but I'm short on time | Paella made with Spanish rice mix, vegetables, and chicken sausage in a slow-cooker | It's made in a slow-cooker so it won't be quick |

### Data Validation

Two full rounds of data validation among curators were conducted. In each round of validation:

- Each subset of 100 queries and the associated labels, options, and descriptions were validated by someone other than the original curator (Validator 1)

- Data was validated to ensure:
  - Queries, options, and text descriptions followed the guidelines established above
  - Correctness and consistency of query type labels
  - A single correct answer can be identified without ambiguity and without looking at the correct answer

- During this validation process, conflicts, confusions, and comments on improvements were recorded by Validator 1

- A second, different individual (Validator 2) then validated the same subset of data and helped resolve conflicts

- Corrections to the data were then made by the individual originally responsible for that subset of data


# Baseline Methods

A number of different evaluation methods were chosen and run on the data to serve as baseline results. These results also helped standardize the difficulty of queries across different individuals. The code for these baselines can be found under baselines/.

## 1) Overlapping Word Count
- [brief description]
- [results - per person and in aggregate]

## 2) TF-IDF Ranking

Using the recipe text descriptions as a corpus, each of the five options were ranked according to TF-IDF scoring against the given query by summing up the TF-IDF scores of each query term.

- The term frequency used was the logarithmically scaled frequency: $tf(t,d) = log(1+f_{t,d})$ where $f_{t,d}$ is the frequency of term $t$ in document $d$ (the text description of the recipe).

- The inverse document frequency used was adjusted to avoid division-by-zero: $idf(t,D) = log(N/(1 + df))$ where $N$ is the total number of recipe descriptions and $df$ is the document frequency of the term $t$ (number of descriptions containing the term).

The accuracy was computed for each query + set of options in aggregate across the whole dataset and individually across different data curators.

[insert overall results in aggregate]

Per-person results:

| Data Curator | H@1 (accuracy) |
| -------- |-------- |
| K | |
| Y | |
| H | 25/100 |
| Z | |
| N | |


## 3) Word Embedding (Glove)

## 4) Neural IR (BERT, TAS-B)

# Evaluation Metrics

- ## H@k, k < 5

- ## MRR