{"cells":[{"cell_type":"code","execution_count":18,"metadata":{"executionInfo":{"elapsed":1878,"status":"ok","timestamp":1666295242444,"user":{"displayName":"Haochen Zhang","userId":"14248919892793833207"},"user_tz":240},"id":"6SruQCNf7qOP"},"outputs":[],"source":["import math\n","import pandas as pd\n","from sklearn.utils import shuffle\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","import numpy as np\n","from sklearn.metrics.pairwise import cosine_similarity\n","import nltk\n","import nltk, nltk.stem, nltk.corpus, nltk.tokenize # if missing downloads, please run downloads below (only need once)\n","from nltk.stem import WordNetLemmatizer\n","from nltk.corpus import stopwords\n","import string\n","import json\n","from nltk.tokenize import word_tokenize"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1180,"status":"ok","timestamp":1666295243603,"user":{"displayName":"Haochen Zhang","userId":"14248919892793833207"},"user_tz":240},"id":"-BGB5phIhilG","outputId":"5193bc50-7005-4757-b3aa-444d4c134bfe"},"outputs":[],"source":["# nltk for data cleaning. Only need to be downloaded once\n","\n","nltk.download('stopwords')\n","nltk.download('punkt')\n","nltk.download('wordnet')\n","nltk.download('omw-1.4')"]},{"cell_type":"code","execution_count":20,"metadata":{"executionInfo":{"elapsed":1283,"status":"ok","timestamp":1666295273054,"user":{"displayName":"Haochen Zhang","userId":"14248919892793833207"},"user_tz":240},"id":"0VMTzOJV_oOB"},"outputs":[],"source":["all_path = \"../data/500QA.json\"\n","with open(all_path) as f:\n","  all_data = json.load(f)"]},{"cell_type":"markdown","metadata":{"id":"rxU_Zp89hbzN"},"source":["Data pre-processing with stopword removal, lemmatization, and punctuation removal"]},{"cell_type":"code","execution_count":21,"metadata":{"executionInfo":{"elapsed":10,"status":"ok","timestamp":1666295273056,"user":{"displayName":"Haochen Zhang","userId":"14248919892793833207"},"user_tz":240},"id":"5P18N3BWjVl9"},"outputs":[],"source":["lemmatizer = WordNetLemmatizer()\n","stop = set(stopwords.words('english') + list(string.punctuation))\n","\n","def clean_text(text):\n","  lst_tokens = [i for i in word_tokenize(text.lower()) if i not in stop]\n","  lemmatized_lst = []\n","  for token in lst_tokens:\n","    lemmatized_token = lemmatizer.lemmatize(token)\n","    lemmatized_lst.append(lemmatized_token)\n","  lemmatized_sentence = \" \".join(lemmatized_lst)\n","  return lemmatized_sentence"]},{"cell_type":"code","execution_count":22,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":639,"status":"ok","timestamp":1666296094334,"user":{"displayName":"Haochen Zhang","userId":"14248919892793833207"},"user_tz":240},"id":"FGQRlfUDWBOE","outputId":"a343262e-a9c2-4762-af2c-0f8396176508"},"outputs":[],"source":["# get flattened list of descriptions for each data sheet\n","\n","all_descriptions = []\n","for d in all_data:\n","  d[\"query\"] = clean_text(d[\"query\"])\n","  for o in d[\"options\"]:\n","    d[\"options\"][o] = clean_text(d[\"options\"][o])\n","    all_descriptions.append(d[\"options\"][o])\n","\n","descriptions = all_descriptions"]},{"cell_type":"code","execution_count":23,"metadata":{"executionInfo":{"elapsed":10,"status":"ok","timestamp":1666296094335,"user":{"displayName":"Haochen Zhang","userId":"14248919892793833207"},"user_tz":240},"id":"2tnFQGt_kEqN"},"outputs":[],"source":["import numpy as np\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from scipy import sparse\n","\n","class BM25(object):\n","    def __init__(self, b=0.75, k1=1.6):\n","        self.vectorizer = TfidfVectorizer(norm=None, smooth_idf=False)\n","        self.b = b\n","        self.k1 = k1\n","\n","    def fit(self, X):\n","        \"\"\" Fit IDF to documents X \"\"\"\n","        self.vectorizer.fit(X)\n","        y = super(TfidfVectorizer, self.vectorizer).transform(X)\n","        self.avdl = y.sum(1).mean()\n","\n","    def transform(self, q, X):\n","        \"\"\" Calculate BM25 between query q and documents X \"\"\"\n","        b, k1, avdl = self.b, self.k1, self.avdl\n","\n","        # apply CountVectorizer\n","        X = super(TfidfVectorizer, self.vectorizer).transform(X)\n","        len_X = X.sum(1).A1\n","        q, = super(TfidfVectorizer, self.vectorizer).transform([q])\n","        assert sparse.isspmatrix_csr(q)\n","\n","        # convert to csc for better column slicing\n","        X = X.tocsc()[:, q.indices]\n","        denom = X + (k1 * (1 - b + b * len_X / avdl))[:, None]\n","        # idf(t) = log [ n / df(t) ] + 1 in sklearn, so it need to be coneverted\n","        # to idf(t) = log [ n / df(t) ] with minus 1\n","        idf = self.vectorizer._tfidf.idf_[None, q.indices] - 1.\n","        numer = X.multiply(np.broadcast_to(idf, X.shape)) * (k1 + 1)\n","        return (numer / denom).sum(1).A1"]},{"cell_type":"code","execution_count":24,"metadata":{"executionInfo":{"elapsed":11,"status":"ok","timestamp":1666296094337,"user":{"displayName":"Haochen Zhang","userId":"14248919892793833207"},"user_tz":240},"id":"iLBSRVMLbAck"},"outputs":[],"source":["def calc_bm25(data, name):\n","  results = []\n","  correct = 0\n","  total = len(data)\n","  bm25 = BM25()\n","  bm25.fit(descriptions)\n","  type_correct = {\n","    \"Specific\": 0,\n","    \"Subjective\": 0,\n","    \"Indirect\": 0,\n","    \"Compound\": 0,\n","    \"Negated\": 0,\n","    \"Analogical\": 0,\n","    \"Temporal\": 0}\n","\n","  for d in data:\n","    try:\n","      options = [val for val in d['options'].values()]\n","      query = d['query']\n","      answer = d['options'][d['answer']]\n","    except:\n","      continue\n","      \n","    options_str = [str(i) for i in options]\n","    doc_scores = bm25.transform(query, options_str)\n","    \n","    # choose option that has highest similarity as correct answer\n","    doc_scores, options = shuffle(doc_scores, options, random_state=0)\n","    ind = np.argmax(doc_scores) \n","    result = 0\n","    if (options[ind]) == answer:\n","      correct += 1\n","      result = 1\n","      for key in d['query_type']:\n","        if d['query_type'][key] == 1:\n","          type_correct[key] += 1\n","    #results.append([result, \"Query: \" + query, \"Recommended: \" + str(options[ind])])\n","\n","  print(\"Results for {}:\".format(name))\n","  print(\"Total correct answers: {} out of {}\".format(correct, total))\n","  #print(results)\n","  print(type_correct)\n","\n","  return correct, total"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1014,"status":"ok","timestamp":1666296095341,"user":{"displayName":"Haochen Zhang","userId":"14248919892793833207"},"user_tz":240},"id":"qJwu98gSS-Nw","outputId":"50968553-12a0-47f0-f5c9-bf051fcd76ab"},"outputs":[],"source":["calc_bm25(all_data, \"all\")"]}],"metadata":{"colab":{"collapsed_sections":[],"provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3.7.3 ('base')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"},"vscode":{"interpreter":{"hash":"16a4e7ece9e04ef6782fa5b68100d2dbb77b79a687737e3689cfa4e040872a6d"}}},"nbformat":4,"nbformat_minor":0}
