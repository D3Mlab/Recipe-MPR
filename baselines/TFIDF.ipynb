{"cells":[{"cell_type":"code","execution_count":50,"metadata":{"executionInfo":{"elapsed":148,"status":"ok","timestamp":1666295603486,"user":{"displayName":"Haochen Zhang","userId":"14248919892793833207"},"user_tz":240},"id":"6SruQCNf7qOP"},"outputs":[],"source":["import math\n","import pandas as pd\n","from sklearn.utils import shuffle\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","import numpy as np\n","from sklearn.metrics.pairwise import cosine_similarity\n","import nltk\n","import nltk, nltk.stem, nltk.corpus, nltk.tokenize # if missing downloads, please run downloads below (only need once)\n","from nltk.stem import WordNetLemmatizer\n","from nltk.corpus import stopwords\n","import string\n","import json\n","from nltk.tokenize import word_tokenize"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":216,"status":"ok","timestamp":1666295603881,"user":{"displayName":"Haochen Zhang","userId":"14248919892793833207"},"user_tz":240},"id":"-BGB5phIhilG","outputId":"5189806a-9bf0-4fdb-8fb7-59f85999231d"},"outputs":[],"source":["# nltk for data cleaning. Only need to be downloaded once\n","\n","nltk.download('stopwords')\n","nltk.download('punkt')\n","nltk.download('wordnet')\n","nltk.download('omw-1.4')"]},{"cell_type":"code","execution_count":52,"metadata":{"executionInfo":{"elapsed":633,"status":"ok","timestamp":1666295627970,"user":{"displayName":"Haochen Zhang","userId":"14248919892793833207"},"user_tz":240},"id":"0VMTzOJV_oOB"},"outputs":[],"source":["all_path = \"../data/500QA.json\"\n","with open(all_path) as f:\n","  all_data = json.load(f)"]},{"cell_type":"markdown","metadata":{"id":"rxU_Zp89hbzN"},"source":["Data pre-processing with stopword removal, lemmatization, and punctuation removal"]},{"cell_type":"code","execution_count":53,"metadata":{"executionInfo":{"elapsed":15,"status":"ok","timestamp":1666295627973,"user":{"displayName":"Haochen Zhang","userId":"14248919892793833207"},"user_tz":240},"id":"5P18N3BWjVl9"},"outputs":[],"source":["lemmatizer = WordNetLemmatizer()\n","stop = set(stopwords.words('english') + list(string.punctuation))\n","\n","def clean_text(text):\n","  lst_tokens = [i for i in word_tokenize(text.lower()) if i not in stop]\n","  lemmatized_lst = []\n","  for token in lst_tokens:\n","    lemmatized_token = lemmatizer.lemmatize(token)\n","    lemmatized_lst.append(lemmatized_token)\n","  lemmatized_sentence = \" \".join(lemmatized_lst)\n","  return lemmatized_sentence"]},{"cell_type":"code","execution_count":54,"metadata":{"executionInfo":{"elapsed":2255,"status":"ok","timestamp":1666295630217,"user":{"displayName":"Haochen Zhang","userId":"14248919892793833207"},"user_tz":240},"id":"FGQRlfUDWBOE"},"outputs":[],"source":["all_descriptions = []\n","for d in all_data:\n","  d[\"query\"] = clean_text(d[\"query\"])\n","  for o in d[\"options\"]:\n","    d[\"options\"][o] = clean_text(d[\"options\"][o])\n","    all_descriptions += d[\"options\"][o]\n","\n","descriptions = all_descriptions"]},{"cell_type":"markdown","metadata":{"id":"P6cAWbq1iriY"},"source":["Calculate document frequency across the entire corpus"]},{"cell_type":"code","execution_count":55,"metadata":{"executionInfo":{"elapsed":19,"status":"ok","timestamp":1666295630220,"user":{"displayName":"Haochen Zhang","userId":"14248919892793833207"},"user_tz":240},"id":"2tnFQGt_kEqN"},"outputs":[],"source":["doc_freq = {}\n","\n","# calculate document frequency for terms based on corpus of descriptions\n","def calc_df(docs):\n","  for doc in docs:\n","    words = doc.split(' ')\n","    words = list(set(words))\n","    for w in words:\n","      if w in doc_freq.keys():\n","        doc_freq[w] += 1\n","      else:\n","        doc_freq.update({w:1})\n","\n","calc_df(descriptions)"]},{"cell_type":"markdown","metadata":{"id":"tVTNXjWhjAzE"},"source":["Choose correct option by summing up tf-idf for query terms"]},{"cell_type":"code","execution_count":56,"metadata":{"executionInfo":{"elapsed":18,"status":"ok","timestamp":1666295630222,"user":{"displayName":"Haochen Zhang","userId":"14248919892793833207"},"user_tz":240},"id":"iLBSRVMLbAck"},"outputs":[],"source":["# General function to compute tf-idf for per-person data (but using entire corpus)\n","\n","def tfidf_score(data, name):\n","  results = []\n","  correct = 0\n","  total = len(data)\n","  type_correct = {\n","    \"Specific\": 0,\n","    \"Subjective\": 0,\n","    \"Indirect\": 0,\n","    \"Compound\": 0,\n","    \"Negated\": 0,\n","    \"Analogical\": 0,\n","    \"Temporal\": 0}\n","    \n","  for d in data:\n","    try:\n","      options = [val for val in d['options'].values()]\n","      query = d['query']\n","      answer = d['options'][d['answer']]\n","    except:\n","      continue\n","      \n","    options_str = [str(i) for i in options]\n","    query_str = query.split(' ')\n","    \n","    doc_scores = []\n","    for option in options_str:\n","      score = 0\n","      # sum over query terms in each document\n","      for term in query_str:\n","        freq = option.count(term)\n","        if freq != 0:\n","          tf = 1 + math.log10(freq)\n","        else:\n","          tf = 0\n","        if term in doc_freq.keys():\n","          idf = math.log10(len(descriptions)/(doc_freq[term] + 1))\n","        else:\n","          idf = math.log10(len(descriptions))\n","        score += tf*idf\n","      doc_scores.append(score)\n","\n","    # choose option that has highest similarity as correct answer\n","    doc_scores, options = shuffle(doc_scores, options, random_state=0)\n","    ind = np.argmax(doc_scores) \n","    result = 0\n","    if (options[ind]) == answer:\n","      correct += 1\n","      result = 1\n","      for key in d['query_type']:\n","        if d['query_type'][key] == 1:\n","          type_correct[key] += 1\n","    #results.append([result, \"Query: \" + query, \"Recommended: \" + str(options[ind])])\n","\n","  print(\"Results for {}:\".format(name))\n","  print(\"Total correct answers: {} out of {}\".format(correct, total))\n","  #print(results)\n","  print(type_correct)\n","\n","  return correct, total"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":198,"status":"ok","timestamp":1666295630404,"user":{"displayName":"Haochen Zhang","userId":"14248919892793833207"},"user_tz":240},"id":"qJwu98gSS-Nw","outputId":"46cafb0a-28fb-4b63-816a-a5d36327c112"},"outputs":[],"source":["tfidf_score(all_data, \"all\")"]}],"metadata":{"colab":{"collapsed_sections":[],"provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3.7.3 ('base')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"},"vscode":{"interpreter":{"hash":"16a4e7ece9e04ef6782fa5b68100d2dbb77b79a687737e3689cfa4e040872a6d"}}},"nbformat":4,"nbformat_minor":0}
