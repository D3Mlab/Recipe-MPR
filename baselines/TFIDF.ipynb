{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6SruQCNf7qOP"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import pandas as pd\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import nltk\n",
        "import nltk, nltk.stem, nltk.corpus, nltk.tokenize # if missing downloads, please run downloads below (only need once)\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "import string\n",
        "from nltk.tokenize import word_tokenize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-BGB5phIhilG",
        "outputId": "0060eb6d-a748-4b8c-dff4-4d1dec479905"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# nltk for data cleaning. Only need to be downloaded once\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0VMTzOJV_oOB"
      },
      "outputs": [],
      "source": [
        "# Load data from csv files, change to json later\n",
        "\n",
        "K_path = \"TFIDF-K.csv\"\n",
        "K_df = pd.read_csv(K_path)\n",
        "Y_path = \"TFIDF-Y.csv\"\n",
        "Y_df = pd.read_csv(Y_path)\n",
        "N_path = \"TFIDF-N.csv\" \n",
        "N_df = pd.read_csv(N_path)\n",
        "H_path = \"TFIDF-H.csv\"\n",
        "H_df = pd.read_csv(H_path)\n",
        "Z_path = \"TFIDF-Z.csv\"\n",
        "Z_df = pd.read_csv(Z_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rxU_Zp89hbzN"
      },
      "source": [
        "Data pre-processing with stopword removal, lemmatization, and punctuation removal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5P18N3BWjVl9"
      },
      "outputs": [],
      "source": [
        "lemmatizer = WordNetLemmatizer()\n",
        "stop = set(stopwords.words('english') + list(string.punctuation))\n",
        "\n",
        "def clean_text(text):\n",
        "  lst_tokens = [i for i in word_tokenize(text.lower()) if i not in stop]\n",
        "  lemmatized_lst = []\n",
        "  for token in lst_tokens:\n",
        "    lemmatized_token = lemmatizer.lemmatize(token)\n",
        "    lemmatized_lst.append(lemmatized_token)\n",
        "  lemmatized_sentence = \" \".join(lemmatized_lst)\n",
        "  return lemmatized_sentence\n",
        "\n",
        "def filter_description(descriptions):\n",
        "  final_desc = []\n",
        "  for description in descriptions:\n",
        "    lst_tokens = [i for i in word_tokenize(description.lower()) if i not in stop]\n",
        "    lemmatized_lst = []\n",
        "    for token in lst_tokens:\n",
        "      lemmatized_token = lemmatizer.lemmatize(token)\n",
        "      lemmatized_lst.append(lemmatized_token)\n",
        "    lemmatized_sentence = \" \".join(lemmatized_lst)\n",
        "    final_desc.append(lemmatized_sentence)\n",
        "  return final_desc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FGQRlfUDWBOE",
        "outputId": "955c9868-029d-44eb-fc73-53df6c41114d"
      },
      "outputs": [],
      "source": [
        "K_df = K_df.dropna()\n",
        "Y_df = Y_df.dropna()\n",
        "H_df = H_df.dropna()\n",
        "Z_df = Z_df.dropna()\n",
        "N_df = N_df.dropna()\n",
        "\n",
        "K_df = K_df.applymap(clean_text)\n",
        "Y_df = Y_df.applymap(clean_text)\n",
        "H_df = H_df.applymap(clean_text)\n",
        "Z_df = Z_df.applymap(clean_text)\n",
        "N_df = N_df.applymap(clean_text)\n",
        "\n",
        "cols = [\"Option 1\", \"Option 2\", \"Option 3\", \"Option 4\", \"Option 5\"]\n",
        "# get flattened list of descriptions for each data sheet\n",
        "K_descriptions = [x for xs in K_df[cols].values for x in xs]\n",
        "Y_descriptions = [x for xs in Y_df[cols].values for x in xs]\n",
        "H_descriptions = [x for xs in H_df[cols].values for x in xs]\n",
        "Z_descriptions = [x for xs in Z_df[cols].values for x in xs]\n",
        "N_descriptions = [x for xs in N_df[cols].values for x in xs]\n",
        "\n",
        "K_cleaned = filter_description(K_descriptions)\n",
        "Y_cleaned = filter_description(Y_descriptions)\n",
        "H_cleaned = filter_description(H_descriptions)\n",
        "Z_cleaned = filter_description(Z_descriptions)\n",
        "N_cleaned = filter_description(N_descriptions)\n",
        "\n",
        "descriptions = K_cleaned + Y_cleaned + H_cleaned + Z_cleaned + N_cleaned"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P6cAWbq1iriY"
      },
      "source": [
        "Calculate document frequency across the entire corpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2tnFQGt_kEqN",
        "outputId": "6d16d4ed-a2ba-4aec-a933-07f49deee3c4"
      },
      "outputs": [],
      "source": [
        "doc_freq = {}\n",
        "\n",
        "# calculate document frequency for terms based on corpus of descriptions\n",
        "def calc_df(docs):\n",
        "  for doc in docs:\n",
        "    words = doc.split(' ')\n",
        "    words = list(set(words))\n",
        "    for w in words:\n",
        "      if w in doc_freq.keys():\n",
        "        doc_freq[w] += 1\n",
        "      else:\n",
        "        doc_freq.update({w:1})\n",
        "\n",
        "calc_df(descriptions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tVTNXjWhjAzE"
      },
      "source": [
        "Choose correct option by summing up tf-idf for query terms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iLBSRVMLbAck"
      },
      "outputs": [],
      "source": [
        "# General function to compute tf-idf for per-person data (but using entire corpus)\n",
        "\n",
        "def tfidf_score(df, name):\n",
        "  results = []\n",
        "  correct = 0\n",
        "  total = len(df)\n",
        "  for i in range(len(df)):\n",
        "    try:\n",
        "      options = [df.loc[i, \"Option 1\"], df.loc[i, \"Option 2\"], df.loc[i, \"Option 3\"], df.loc[i, \"Option 4\"], df.loc[i, \"Option 5\"]]\n",
        "      query = df.loc[i, \"Query\"]\n",
        "      answer = df.loc[i, \"Correct Answer\"]\n",
        "    except:\n",
        "      continue\n",
        "      \n",
        "    options_str = [str(i) for i in options]\n",
        "    query_str = query.split(' ')\n",
        "    \n",
        "    doc_scores = []\n",
        "    for option in options_str:\n",
        "      score = 0\n",
        "      # sum over query terms in each document\n",
        "      for term in query_str:\n",
        "        freq = option.count(term)\n",
        "        if freq != 0:\n",
        "          tf = 1 + math.log10(freq)\n",
        "        else:\n",
        "          tf = 0\n",
        "        if term in doc_freq.keys():\n",
        "          idf = math.log10(len(descriptions)/(doc_freq[term] + 1))\n",
        "        else:\n",
        "          idf = math.log10(len(descriptions))\n",
        "        score += tf*idf\n",
        "      doc_scores.append(score)\n",
        "\n",
        "    # choose option that has highest similarity as correct answer\n",
        "    doc_scores, options = shuffle(doc_scores, options, random_state=0)\n",
        "    ind = np.argmax(doc_scores) \n",
        "    result = 0\n",
        "    if (options[ind]) == answer:\n",
        "      correct += 1\n",
        "      result = 1\n",
        "    results.append([result, \"Query: \" + str(df.loc[i, \"Query\"]), \"Recommended: \" + str(options[ind])])\n",
        "\n",
        "  print(\"Results for {}:\".format(name))\n",
        "  print(\"Total correct answers: {} out of {}\".format(correct, total))\n",
        "  print(results)\n",
        "\n",
        "  return correct, total"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qJwu98gSS-Nw",
        "outputId": "533be289-109f-44b8-964d-125374a6b7da"
      },
      "outputs": [],
      "source": [
        "# Individual results\n",
        "\n",
        "tfidf_score(K_df, \"K\")\n",
        "tfidf_score(Y_df, \"Y\")\n",
        "tfidf_score(H_df, \"H\")\n",
        "tfidf_score(Z_df, \"Z\")\n",
        "tfidf_score(N_df, \"N\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "TFIDF.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
