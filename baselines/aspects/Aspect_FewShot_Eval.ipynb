{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jDYxoBBzO7Bp"
      },
      "outputs": [],
      "source": [
        "!pip install transformers\n",
        "!pip install sentencepiece"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rkkjpw8zCQ06",
        "outputId": "b26ff66c-b4f3-4173-8c88-a5fb29ce950e"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.append('..')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "OW1rCoFP7TfO"
      },
      "outputs": [],
      "source": [
        "from helper import *\n",
        "import torch\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel, AutoTokenizer\n",
        "from transformers import OpenAIGPTTokenizer, OpenAIGPTLMHeadModel, OPTForCausalLM, GPT2Tokenizer\n",
        "from sklearn.utils import shuffle\n",
        "from torch import optim\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "import json\n",
        "import csv\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "8LRIqYOKyupj"
      },
      "outputs": [],
      "source": [
        "config = load_config()\n",
        "train_splits, val_splits, test_splits = load_data(config)\n",
        "\n",
        "agg_fcns = {\"min\":min, \"max\":max, \"amean\":np.mean, \"gmean\":custom_gmean}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "mSgE30ypCXQ7"
      },
      "outputs": [],
      "source": [
        "# for generating set prompts from ground-truth aspects\n",
        "def few_shot_prompt(q, add_answer=False):\n",
        "  text = \"\"\n",
        "  for key in list(q['correctness_explanation'].keys()):\n",
        "    text += \"input: \" + str(key) + ' \\n'\n",
        "    text += \"output: \"\n",
        "    if add_answer:\n",
        "      text += q['options'][q['answer']] + \" \\n\"\n",
        "\n",
        "  return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "kH5qe62ECZqH"
      },
      "outputs": [],
      "source": [
        "def get_fewshot_NLL_score(model,tokenizer,condition,text,filler=' so I recommend ',normalize=False):\n",
        "  text = condition + filler  + text\n",
        "  encodings = tokenizer(text, return_tensors=\"pt\")\n",
        "  condition = tokenizer(condition, return_tensors=\"pt\")\n",
        "  stride = condition.input_ids.size(1)\n",
        "\n",
        "  nlls = []\n",
        "\n",
        "  begin_loc = 0\n",
        "  end_loc = stride\n",
        "  trg_len = encodings.input_ids.size(1) - stride\n",
        "  input_ids = encodings.input_ids.to('cuda')\n",
        "  target_ids = input_ids.clone()\n",
        "\n",
        "  with torch.no_grad():\n",
        "      outputs = model(input_ids, labels=target_ids)\n",
        "      neg_log_likelihood = outputs[0] \n",
        "  \n",
        "  if normalize:\n",
        "      with torch.no_grad():\n",
        "        c_input_ids = condition.input_ids.to('cuda')\n",
        "        outputs = model(c_input_ids, labels=c_input_ids)\n",
        "        c_neg_log_likelihood = outputs[0]\n",
        "      return (-1 * neg_log_likelihood) - (-1 * c_neg_log_likelihood)\n",
        "  else:\n",
        "    return -1 * neg_log_likelihood\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "vR9rgKZpCmnx"
      },
      "outputs": [],
      "source": [
        "def aspect_fewshot(dataset, model, tokenizer, normalize, prompt, agg_fcn):\n",
        "  type_correct = {\n",
        "      \"Specific\": 0,\n",
        "      \"Subjective\": 0,\n",
        "      \"Commonsense\": 0,\n",
        "      \"Compound\": 0,\n",
        "      \"Negated\": 0,\n",
        "      \"Analogical\": 0,\n",
        "      \"Temporal\": 0\n",
        "    }\n",
        "  type_count = {\n",
        "      \"Specific\": 0,\n",
        "      \"Subjective\": 0,\n",
        "      \"Commonsense\": 0,\n",
        "      \"Compound\": 0,\n",
        "      \"Negated\": 0,\n",
        "      \"Analogical\": 0,\n",
        "      \"Temporal\": 0\n",
        "    }\n",
        "  # number of correct predictions\n",
        "  correct = 0\n",
        "  # h@1 evaluation metric\n",
        "  total_hit_at_1 = 0\n",
        "  # number of queries\n",
        "  count = 0\n",
        "  output_message = \"\"\n",
        "\n",
        "  # loop through each query\n",
        "  for sample in dataset:\n",
        "    count += 1\n",
        "    if count % 50 == 0:\n",
        "      print('--> ',count)\n",
        "    \n",
        "    for key in sample['query_type']:\n",
        "        if sample['query_type'][key] == 1:\n",
        "          type_count[key] += 1\n",
        "\n",
        "    output_message += str(count) + ' Query: ' + sample[\"query\"] + ' \\n'\n",
        "    \n",
        "    q_text = sample[\"query\"]\n",
        "    aspects = sample[\"correctness_explanation\"].keys()\n",
        "    options_list = [val for val in sample[\"options\"].values()]\n",
        "\n",
        "    all_scores = []\n",
        "    for a in aspects: \n",
        "      p =\"input: \" + a + \"\\n\"\n",
        "      q_text = prompt + p\n",
        "      scores = []\n",
        "\n",
        "      for key in sample[\"options\"]:\n",
        "        score = get_fewshot_NLL_score(model, tokenizer, q_text, sample[\"options\"][key], normalize=normalize, filler='')\n",
        "        assert not torch.isnan(score), 'score is nan'\n",
        "        scores.append(float(score))\n",
        "\n",
        "        if key == sample[\"answer\"]:\n",
        "          output_message += 'Answer: ' + str(score) + ' ' + sample[\"options\"][key] + ' \\n'\n",
        "        else:\n",
        "          output_message += str(score) + ' ' + sample[\"options\"][key] + ' \\n'\n",
        "\n",
        "      all_scores.append(scores)\n",
        "\n",
        "    agg_scores = aggregate(all_scores, agg_fcn)\n",
        "    agg_scores, options_list = shuffle(agg_scores, options_list, random_state=0)\n",
        "    args = np.argsort(agg_scores)\n",
        "    predicted_id = options_list[args[-1]]\n",
        "\n",
        "    ## check if predicted id is the same as correct id\n",
        "    if predicted_id == sample[\"options\"][sample[\"answer\"]]:\n",
        "      output_message += 'True \\n'\n",
        "      correct += 1\n",
        "      total_hit_at_1 += 1\n",
        "      \n",
        "      # count number of correct queries for each type\n",
        "      for key in sample['query_type']:\n",
        "        if sample['query_type'][key] == 1:\n",
        "          type_correct[key] += 1\n",
        "    else:\n",
        "      output_message+='False: ' + predicted_id +'\\n'\n",
        "    \n",
        "    output_message+='-'*10 + ' \\n'\n",
        "\n",
        "  return correct, count, type_correct, type_count"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "YRpC2-gXbdgO"
      },
      "outputs": [],
      "source": [
        "def evaluate(name, model, tokenizer, prompt_size=5):\n",
        "\n",
        "  for agg_name, agg_fcn in agg_fcns.items():\n",
        "    results_file = \"FewShot_\" + name + \"_\" + agg_name + \"_\" + str(prompt_size) + \".csv\"\n",
        "    for i in range(len(train_splits)):\n",
        "      print(\"Trial: \", i)\n",
        "      prompt = ''\n",
        "\n",
        "      # generate prompt sample\n",
        "      for index in range(prompt_size):\n",
        "        p = few_shot_prompt(train_splits[i][index],True)\n",
        "        prompt += p\n",
        "      \n",
        "      print(\"Prompt:\")\n",
        "      print(prompt)\n",
        "      correct, count, type_correct, type_count = aspect_fewshot(test_splits[i], model, tokenizer, normalize=True, prompt=prompt, agg_fcn=agg_fcn)\n",
        "      results.append(correct)\n",
        "      for key, val in type_correct.items():\n",
        "          type_correct[key] = val*100/type_count[key]\n",
        "        type_correct.update({\"All\":correct*100/count})\n",
        "\n",
        "      with open(results_file, \"a\") as f:\n",
        "        writer = csv.writer(f)\n",
        "        writer.writerow([i, type_correct['All'], type_correct[\"Analogical\"], \n",
        "                        type_correct[\"Commonsense\"], type_correct[\"Compound\"], \n",
        "                        type_correct[\"Negated\"], type_correct[\"Specific\"],\n",
        "                        type_correct[\"Subjective\"], type_correct[\"Temporal\"]])\n",
        "      \n",
        "      print(\"Total correct: {} out of {}\".format(correct, count))\n",
        "\n",
        "    with open(results_file, \"a\") as f:\n",
        "        writer = csv.writer(f)\n",
        "        writer.writerow([\"Total\", count, type_count[\"Analogical\"], \n",
        "                        type_count[\"Commonsense\"], type_count[\"Compound\"], \n",
        "                        type_count[\"Negated\"], type_count[\"Specific\"],\n",
        "                        type_count[\"Subjective\"], type_count[\"Temporal\"]])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d0gXCjHev8Qg"
      },
      "outputs": [],
      "source": [
        "prompt_size = config['prompt_size']\n",
        "\n",
        "model_name = 'facebook/opt-1.3b'\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = OPTForCausalLM.from_pretrained(model_name).cuda()\n",
        "\n",
        "evaluate(\"OPT-1.3b\", model, tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Chlc0sXq4CpP"
      },
      "outputs": [],
      "source": [
        "model_name = 'gpt-2'\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "model = GPT2LMHeadModel.from_pretrained(model_name).cuda()\n",
        "\n",
        "evaluate(\"GPT-2\", model, tokenizer)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.3 (default, Mar 27 2019, 17:13:21) [MSC v.1915 64 bit (AMD64)]"
    },
    "vscode": {
      "interpreter": {
        "hash": "16a4e7ece9e04ef6782fa5b68100d2dbb77b79a687737e3689cfa4e040872a6d"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
