{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jDYxoBBzO7Bp"
      },
      "outputs": [],
      "source": [
        "!pip install transformers\n",
        "!pip install sentencepiece"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rkkjpw8zCQ06",
        "outputId": "b26ff66c-b4f3-4173-8c88-a5fb29ce950e"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.append('..')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "OW1rCoFP7TfO"
      },
      "outputs": [],
      "source": [
        "from helper import *\n",
        "import torch\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel, AutoTokenizer\n",
        "from transformers import OpenAIGPTTokenizer, OpenAIGPTLMHeadModel, OPTForCausalLM, GPT2Tokenizer\n",
        "from sklearn.utils import shuffle\n",
        "from torch import optim\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "import json\n",
        "import csv\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "8LRIqYOKyupj"
      },
      "outputs": [],
      "source": [
        "config = load_config()\n",
        "train_splits, val_splits, test_splits = load_data(config)\n",
        "\n",
        "agg_fcns = {\"min\":min, \"max\":max, \"amean\":np.mean, \"gmean\":custom_gmean}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "mSgE30ypCXQ7"
      },
      "outputs": [],
      "source": [
        "# for generating set prompts from ground-truth aspects\n",
        "def few_shot_prompt(q, add_answer=False):\n",
        "\ttext = \"\"\n",
        "\tfor key in list(q['correctness_explanation'].keys()):\n",
        "\t\ttext += \"input: \" + str(key) + ' \\n'\n",
        "\t\ttext += \"output: \"\n",
        "\t\tif add_answer:\n",
        "\t\t\ttext += q['options'][q['answer']] + \" \\n\"\n",
        "\n",
        "\treturn text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "kH5qe62ECZqH"
      },
      "outputs": [],
      "source": [
        "def get_fewshot_NLL_score(model,tokenizer,condition,text,filler=' so I recommend ',normalize=False):\n",
        "\ttext = condition + filler  + text\n",
        "\tencodings = tokenizer(text, return_tensors=\"pt\")\n",
        "\tcondition = tokenizer(condition, return_tensors=\"pt\")\n",
        "\tstride = condition.input_ids.size(1)\n",
        "\n",
        "\tnlls = []\n",
        "\n",
        "\tbegin_loc = 0\n",
        "\tend_loc = stride\n",
        "\ttrg_len = encodings.input_ids.size(1) - stride\n",
        "\tinput_ids = encodings.input_ids.to('cuda')\n",
        "\ttarget_ids = input_ids.clone()\n",
        "\n",
        "\twith torch.no_grad():\n",
        "\t\toutputs = model(input_ids, labels=target_ids)\n",
        "\t\tneg_log_likelihood = outputs[0] \n",
        "\n",
        "\tif normalize:\n",
        "\t\twith torch.no_grad():\n",
        "\t\t\tc_input_ids = condition.input_ids.to('cuda')\n",
        "\t\t\toutputs = model(c_input_ids, labels=c_input_ids)\n",
        "\t\t\tc_neg_log_likelihood = outputs[0]\n",
        "\t\treturn (-1 * neg_log_likelihood) - (-1 * c_neg_log_likelihood)\n",
        "\telse:\n",
        "\t\treturn -1 * neg_log_likelihood\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "vR9rgKZpCmnx"
      },
      "outputs": [],
      "source": [
        "def aspect_fewshot(dataset, model, tokenizer, normalize, prompt, agg_fcn):\n",
        "\ttype_correct = {\n",
        "\t\t\"Specific\": 0,\n",
        "\t\t#\"Subjective\": 0,\n",
        "\t\t\"Commonsense\": 0,\n",
        "\t\t#\"Compound\": 0,\n",
        "\t\t\"Negated\": 0,\n",
        "\t\t\"Analogical\": 0,\n",
        "\t\t\"Temporal\": 0\n",
        "\t}\n",
        "\ttype_count = {\n",
        "\t\t\"Specific\": 0,\n",
        "\t\t#\"Subjective\": 0,\n",
        "\t\t\"Commonsense\": 0,\n",
        "\t\t#\"Compound\": 0,\n",
        "\t\t\"Negated\": 0,\n",
        "\t\t\"Analogical\": 0,\n",
        "\t\t\"Temporal\": 0\n",
        "\t}\n",
        "\t# number of correct predictions\n",
        "\tcorrect = 0\n",
        "\t# h@1 evaluation metric\n",
        "\ttotal_hit_at_1 = 0\n",
        "\t# number of queries\n",
        "\tcount = 0\n",
        "\toutput_message = \"\"\n",
        "\n",
        "\t# loop through each query\n",
        "\tfor sample in dataset:\n",
        "\t\tcount += 1\n",
        "\t\tif count % 50 == 0:\n",
        "\t\t\tprint('--> ',count)\n",
        "\n",
        "\t\tfor key in sample['query_type']:\n",
        "\t\t\tif sample['query_type'][key] == 1:\n",
        "\t\t\t\ttype_count[key] += 1\n",
        "\n",
        "\t\toutput_message += str(count) + ' Query: ' + sample[\"query\"] + ' \\n'\n",
        "\n",
        "\t\tq_text = sample[\"query\"]\n",
        "\t\taspects = sample[\"correctness_explanation\"].keys()\n",
        "\t\toptions_list = [val for val in sample[\"options\"].values()]\n",
        "\n",
        "\t\tall_scores = []\n",
        "\t\tfor a in aspects: \n",
        "\t\t\tp =\"input: \" + a + \"\\n\"\n",
        "\t\t\tq_text = prompt + p\n",
        "\t\t\tscores = []\n",
        "\n",
        "\t\t\tfor key in sample[\"options\"]:\n",
        "\t\t\t\tscore = get_fewshot_NLL_score(model, tokenizer, q_text, sample[\"options\"][key], normalize=normalize, filler='')\n",
        "\t\t\t\tassert not torch.isnan(score), 'score is nan'\n",
        "\t\t\t\tscores.append(float(score))\n",
        "\n",
        "\t\t\t\tif key == sample[\"answer\"]:\n",
        "\t\t\t\t\toutput_message += 'Answer: ' + str(score) + ' ' + sample[\"options\"][key] + ' \\n'\n",
        "\t\t\t\telse:\n",
        "\t\t\t\t\toutput_message += str(score) + ' ' + sample[\"options\"][key] + ' \\n'\n",
        "\n",
        "\t\t\tall_scores.append(scores)\n",
        "\n",
        "\t\tagg_scores = aggregate(all_scores, agg_fcn)\n",
        "\t\tagg_scores, options_list = shuffle(agg_scores, options_list, random_state=0)\n",
        "\t\targs = np.argsort(agg_scores)\n",
        "\t\tpredicted_id = options_list[args[-1]]\n",
        "\n",
        "\t\t## check if predicted id is the same as correct id\n",
        "\t\tif predicted_id == sample[\"options\"][sample[\"answer\"]]:\n",
        "\t\t\toutput_message += 'True \\n'\n",
        "\t\t\tcorrect += 1\n",
        "\t\t\ttotal_hit_at_1 += 1\n",
        "\t\t\t\n",
        "\t\t\t# count number of correct queries for each type\n",
        "\t\t\tfor key in sample['query_type']:\n",
        "\t\t\t\tif sample['query_type'][key] == 1:\n",
        "\t\t\t\t\ttype_correct[key] += 1\n",
        "\t\telse:\n",
        "\t\t\toutput_message+='False: ' + predicted_id +'\\n'\n",
        "\n",
        "\t\toutput_message+='-'*10 + ' \\n'\n",
        "\n",
        "\treturn correct, count, type_correct, type_count"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "YRpC2-gXbdgO"
      },
      "outputs": [],
      "source": [
        "def evaluate(name, model, tokenizer, prompt_size=5):\n",
        "\n",
        "\tfor agg_name, agg_fcn in agg_fcns.items():\n",
        "\t\tresults_file = \"FewShot_\" + name + \"_\" + agg_name + \"_\" + str(prompt_size) + \".csv\"\n",
        "\t\tfor i in range(len(train_splits)):\n",
        "\t\t\tprint(\"Trial: \", i)\n",
        "\t\t\tprompt = ''\n",
        "\n",
        "\t\t\t# generate prompt sample\n",
        "\t\t\tfor index in range(prompt_size):\n",
        "\t\t\t\tp = few_shot_prompt(train_splits[i][index],True)\n",
        "\t\t\t\tprompt += p\n",
        "\t\t\t\n",
        "\t\t\tprint(\"Prompt:\")\n",
        "\t\t\tprint(prompt)\n",
        "\t\t\tcorrect, count, type_correct, type_count = aspect_fewshot(test_splits[i], model, tokenizer, normalize=True, prompt=prompt, agg_fcn=agg_fcn)\n",
        "\t\t\tresults.append(correct)\n",
        "\t\t\tfor key, val in type_correct.items():\n",
        "\t\t\t\ttype_correct[key] = val*100/type_count[key]\n",
        "\t\t\ttype_correct.update({\"All\":correct*100/count})\n",
        "\n",
        "\t\t\twith open(results_file, \"a\") as f:\n",
        "\t\t\t\twriter = csv.writer(f)\n",
        "\t\t\t\twriter.writerow([i, type_correct['All'], type_correct[\"Analogical\"], \n",
        "\t\t\t\t\t\t\t\ttype_correct[\"Commonsense\"], type_correct[\"Compound\"], \n",
        "\t\t\t\t\t\t\t\ttype_correct[\"Negated\"], type_correct[\"Specific\"],\n",
        "\t\t\t\t\t\t\t\ttype_correct[\"Subjective\"], type_correct[\"Temporal\"]])\n",
        "\t\t\t\n",
        "\t\t\tprint(\"Total correct: {} out of {}\".format(correct, count))\n",
        "\n",
        "\t\twith open(results_file, \"a\") as f:\n",
        "\t\t\twriter = csv.writer(f)\n",
        "\t\t\twriter.writerow([\"Total\", count, type_count[\"Analogical\"], \n",
        "\t\t\t\t\t\t\ttype_count[\"Commonsense\"], type_count[\"Compound\"], \n",
        "\t\t\t\t\t\t\ttype_count[\"Negated\"], type_count[\"Specific\"],\n",
        "\t\t\t\t\t\t\ttype_count[\"Subjective\"], type_count[\"Temporal\"]])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d0gXCjHev8Qg"
      },
      "outputs": [],
      "source": [
        "prompt_size = config['prompt_size']\n",
        "\n",
        "model_name = 'facebook/opt-1.3b'\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = OPTForCausalLM.from_pretrained(model_name).cuda()\n",
        "\n",
        "evaluate(\"OPT-1.3b\", model, tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Chlc0sXq4CpP"
      },
      "outputs": [],
      "source": [
        "model_name = 'gpt-2'\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "model = GPT2LMHeadModel.from_pretrained(model_name).cuda()\n",
        "\n",
        "evaluate(\"GPT-2\", model, tokenizer)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.3 (default, Mar 27 2019, 17:13:21) [MSC v.1915 64 bit (AMD64)]"
    },
    "vscode": {
      "interpreter": {
        "hash": "16a4e7ece9e04ef6782fa5b68100d2dbb77b79a687737e3689cfa4e040872a6d"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
